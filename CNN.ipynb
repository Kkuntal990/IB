{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBXDPxVXYwrD"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#import plotcm\n",
        "import sys\n",
        "import tensorflow.keras as keras\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Conv2D, ZeroPadding2D\n",
        "from tensorflow.keras.layers import Reshape, Dense, Dropout, Activation, Flatten\n",
        "import tensorflow.keras.models as models\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UTbXN6Zbkf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98762cd8-f2fc-4b10-db2c-27b9e6f6108b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXDfrpJQaWj1"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/IB/RML2016.10a_dict.dat\" /content/"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JQj5IZgYx-u"
      },
      "source": [
        "#import cPickle\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "sys.path.append('../confusion')\n",
        "\n",
        "name = 'CNN2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm0o0AKrYund"
      },
      "source": [
        "\n",
        "with open(\"RML2016.10a_dict.dat\", 'rb') as xd1:  \n",
        "    Xd = pickle.load(xd1, encoding='latin1')  # , encoding='latin1'\n",
        "snrs, mods = map(lambda j: sorted(\n",
        "    list(set(map(lambda x: x[j], Xd.keys())))), [1, 0])\n",
        "X = []\n",
        "lbl = []\n",
        "for mod in mods:\n",
        "    for snr in snrs:\n",
        "        X.append(Xd[(mod, snr)])\n",
        "        for i in range(Xd[(mod, snr)].shape[0]):\n",
        "            lbl.append((mod, snr))\n",
        "X = np.vstack(X)\n",
        "# %%\n",
        "np.random.seed(2016)  \n",
        "n_examples = X.shape[0]\n",
        "n_train = n_examples * 0.5  # 对半\n",
        "train_idx = np.random.choice(\n",
        "    range(0, n_examples), size=int(n_train), replace=False)\n",
        "test_idx = list(set(range(0, n_examples)) - set(train_idx))  # label\n",
        "X_train = X[train_idx]\n",
        "X_test = X[test_idx]\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzrJmCqJY5QA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c25b908-74c5-4bd1-e1eb-cf373fb147e5"
      },
      "source": [
        "def to_onehot(yy):\n",
        "    yy1 = np.zeros([len(yy), max(yy) + 1])\n",
        "    yy1[np.arange(len(yy)), yy] = 1\n",
        "    return yy1\n",
        "\n",
        "\n",
        "trainy = list(map(lambda x: mods.index(lbl[x][0]), train_idx))\n",
        "Y_train = to_onehot(trainy)\n",
        "Y_test = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))\n",
        "# %%\n",
        "in_shp = list(X_train.shape[1:])\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(Y_train.shape, Y_test.shape)\n",
        "classes = mods\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(110000, 2, 128) (110000, 2, 128)\n",
            "(110000, 11) (110000, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MUDQtBfZGh8"
      },
      "source": [
        "def sample_z(args):\n",
        "  mu, sigma = args\n",
        "  batch = K.shape(mu)[0]\n",
        "  dim = K.int_shape(mu)[1]\n",
        "  eps = K.random_normal(shape=(batch, dim))\n",
        "  return mu + K.log(1 + K.exp(sigma - 5.0)) * eps\n",
        "\n",
        "BETA = 0.2\n",
        "prior = K.random_normal(shape=(128,))\n",
        "\n",
        "def custom_loss(layer):\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        class_loss = keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "        info_loss = BETA*keras.losses.kullback_leibler_divergence(layer, prior)\n",
        "        return class_loss + info_loss\n",
        "    \n",
        "    return loss\n",
        "\n",
        "dr = 0.5"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts2CDTmTY-iZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b10ff59-a9d4-44a2-e23e-f0524ac9065a"
      },
      "source": [
        "inp = keras.Input(shape=(1,2,128,))\n",
        "x = ZeroPadding2D((0,2), data_format=\"channels_first\")(inp)\n",
        "x = Conv2D(256, (1, 3), padding='valid', activation=\"relu\", name=\"conv1\", data_format=\"channels_first\")(x)\n",
        "x = Dropout(dr)(x)\n",
        "x = ZeroPadding2D((0, 2), data_format=\"channels_first\")(x)\n",
        "x = Conv2D(80, (2, 3), padding=\"valid\", activation=\"relu\", name=\"conv2\", data_format=\"channels_first\")(x)\n",
        "x = Dropout(dr)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu', name=\"dense1\")(x)\n",
        "x = Dropout(dr)(x)\n",
        "\n",
        "mu, sigma = x[:, :128], x[:, 128:]\n",
        "z = keras.layers.Lambda(sample_z, output_shape=(128, ), name='z')([mu, sigma])\n",
        "y = Dense(len(classes), name=\"dense2\")(z)\n",
        "y = Activation('softmax')(y)\n",
        "\n",
        "model = keras.Model(inp, y, name='VIB')\n",
        "model.summary()\n",
        "model.compile(\"adam\" , \"mse\")\n",
        "epochs = 100  # number of epochs to train on\n",
        "batch_size = 1024  # training batch size default1024\n",
        "# %%\n",
        "filepath = \"convmodrecnets_%s_0.5.wts.h5\" % (\n",
        "    name) \n",
        "history = model.fit(X_train,\n",
        "                    Y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=2,\n",
        "                    validation_data=(X_test, Y_test),\n",
        "                    callbacks=[ \n",
        "                        keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True,\n",
        "                                                        mode='auto'),\n",
        "                        keras.callbacks.EarlyStopping(\n",
        "                            monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
        "                    ])  \n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"VIB\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_22 (InputLayer)           [(None, 1, 2, 128)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_28 (ZeroPadding2 (None, 1, 2, 132)    0           input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 256, 2, 130)  1024        zero_padding2d_28[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 256, 2, 130)  0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_29 (ZeroPadding2 (None, 256, 2, 134)  0           dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2 (Conv2D)                  (None, 80, 1, 132)   122960      zero_padding2d_29[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 80, 1, 132)   0           conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 10560)        0           dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense1 (Dense)                  (None, 256)          2703616     flatten_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 256)          0           dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_17 (T [(None, 128)]        0           dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_18 (T [(None, 128)]        0           dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "z (Lambda)                      (None, 128)          0           tf_op_layer_strided_slice_17[0][0\n",
            "                                                                 tf_op_layer_strided_slice_18[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dense2 (Dense)                  (None, 11)           1419        z[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 11)           0           dense2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,829,019\n",
            "Trainable params: 2,829,019\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-d8bb1595fc43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                         mode='auto'),\n\u001b[1;32m     34\u001b[0m                         keras.callbacks.EarlyStopping(\n\u001b[0;32m---> 35\u001b[0;31m                             monitor='val_loss', patience=5, verbose=0, mode='auto')\n\u001b[0m\u001b[1;32m     36\u001b[0m                     ])  \n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2469\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2471\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2473\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    561\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_22 to have 4 dimensions, but got array with shape (110000, 2, 128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef6MhrvuZo2H"
      },
      "source": [
        "# Show loss curves\n",
        "plt.figure()\n",
        "plt.title('Training performance')\n",
        "plt.plot(history.epoch, history.history['loss'], label='train loss+error')\n",
        "plt.plot(history.epoch, history.history['val_loss'], label='val_error')\n",
        "plt.legend()\n",
        "plt.savefig('%s Training performance' % (name))\n",
        "# plt.show()\n",
        "\n",
        "model.load_weights(filepath)\n",
        "score = model.evaluate(X_test, Y_test, verbose=0, batch_size=batch_size)\n",
        "print('evaluate_score:', score)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(labels))\n",
        "    plt.xticks(tick_marks, labels, rotation=45)\n",
        "    plt.yticks(tick_marks, labels)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.savefig(title)\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "# Plot confusion matrix\n",
        "test_Y_hat = model.predict(X_test, batch_size=batch_size)\n",
        "\n",
        "# %%调用库产生混淆矩阵\n",
        "pre_labels = []\n",
        "for x in test_Y_hat:\n",
        "    tmp = np.argmax(x, 0)\n",
        "    pre_labels.append(tmp)\n",
        "true_labels = []\n",
        "for x in Y_test:\n",
        "    tmp = np.argmax(x, 0)\n",
        "    true_labels.append(tmp)\n",
        "\n",
        "kappa = cohen_kappa_score(pre_labels, true_labels)\n",
        "oa = accuracy_score(true_labels, pre_labels)\n",
        "kappa_oa = {}\n",
        "print('oa_all:', oa)\n",
        "print('kappa_all:', kappa)\n",
        "kappa_oa['oa_all'] = oa\n",
        "kappa_oa['kappa_all'] = kappa\n",
        "fd = open('results_all_%s_d0.5.dat' % (name), 'wb')\n",
        "cPickle.dump((\"%s\" % (name), 0.5, kappa_oa), fd)\n",
        "fd.close()\n",
        "cnf_matrix = confusion_matrix(true_labels, pre_labels)\n",
        "# np.set_printoptions(precision=2)\n",
        "# Plot non-normalized confusion matrix\n",
        "# plt.figure()\n",
        "plotcm.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                             normalize=False,\n",
        "                             title='%s Confusion matrix, without normalization' % (name), showtext=True)\n",
        "plt.savefig('%s Confusion matrix, without normalization' % (name))\n",
        "# Plot normalized confusion matrix\n",
        "# plt.figure()\n",
        "plotcm.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                             normalize=True,\n",
        "                             title='%s Normalized confusion matrix' % (name), showtext=True)\n",
        "plt.savefig('%s Normalized confusion matrix' % (name))\n",
        "# plt.show()\n",
        "\n",
        "# %%自定义产生混淆矩阵\n",
        "conf = np.zeros([len(classes), len(classes)])\n",
        "confnorm = np.zeros([len(classes), len(classes)])\n",
        "for i in range(0, X_test.shape[0]):\n",
        "    j = list(Y_test[i, :]).index(1)\n",
        "    k = int(np.argmax(test_Y_hat[i, :]))\n",
        "    conf[j, k] += 1\n",
        "for i in range(0, len(classes)):\n",
        "    confnorm[i, :] = conf[i, :] / np.sum(conf[i, :])\n",
        "plot_confusion_matrix(confnorm, labels=classes,\n",
        "                      title='%s Confusion matrix' % (name))\n",
        "\n",
        "# %%Plot confusion matrix 画图\n",
        "acc = {}\n",
        "kappa_dict = {}\n",
        "oa_dict = {}\n",
        "for snr in snrs:\n",
        "\n",
        "    # extract classes @ SNR\n",
        "    test_SNRs = list(map(lambda x: lbl[x][1], test_idx))\n",
        "    test_X_i = X_test[np.where(np.array(test_SNRs) == snr)]\n",
        "    test_Y_i = Y_test[np.where(np.array(test_SNRs) == snr)]\n",
        "\n",
        "    # estimate classes\n",
        "    test_Y_i_hat = model.predict(test_X_i)\n",
        "\n",
        "    # %%调用库产生混淆矩阵\n",
        "    pre_labels_i = []\n",
        "    for x in test_Y_i_hat:\n",
        "        tmp = np.argmax(x, 0)\n",
        "        pre_labels_i.append(tmp)\n",
        "    true_labels_i = []\n",
        "    for x in test_Y_i:\n",
        "        tmp = np.argmax(x, 0)\n",
        "        true_labels_i.append(tmp)\n",
        "    kappa = cohen_kappa_score(pre_labels_i, true_labels_i)\n",
        "    oa = accuracy_score(true_labels_i, pre_labels_i)\n",
        "    oa_dict[snr] = oa\n",
        "    kappa_dict[snr] = kappa\n",
        "    cnf_matrix = confusion_matrix(true_labels_i, pre_labels_i)\n",
        "    # np.set_printoptions(precision=2)\n",
        "    # Plot non-normalized confusion matrix\n",
        "    # plt.figure()\n",
        "    plotcm.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                                 normalize=False,\n",
        "                                 title='%s Confusion matrix, without normalization (SNR=%d)' % (name, snr), showtext=True)\n",
        "    plt.savefig('%s Confusion matrix, without normalization (SNR=%d)' %\n",
        "                (name, snr))\n",
        "    # Plot normalized confusion matrix\n",
        "    # plt.figure()\n",
        "    plotcm.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                                 normalize=True,\n",
        "                                 title='%s Normalized confusion matrix (SNR=%d)' % (name, snr), showtext=True)\n",
        "    plt.savefig('%s Normalized confusion matrix (SNR=%d)' % (name, snr))\n",
        "    # plt.show()\n",
        "\n",
        "    # %%自定义产生混淆矩阵\n",
        "    conf = np.zeros([len(classes), len(classes)])\n",
        "    confnorm = np.zeros([len(classes), len(classes)])\n",
        "    for i in range(0, test_X_i.shape[0]):\n",
        "        j = list(test_Y_i[i, :]).index(1)\n",
        "        k = int(np.argmax(test_Y_i_hat[i, :]))\n",
        "        conf[j, k] += 1\n",
        "    for i in range(0, len(classes)):\n",
        "        confnorm[i, :] = conf[i, :] / np.sum(conf[i, :])\n",
        "    # plt.figure()\n",
        "    plot_confusion_matrix(confnorm, labels=classes,\n",
        "                          title=\"%s Confusion Matrix (SNR=%d)\" % (name, snr))\n",
        "\n",
        "    cor = np.sum(np.diag(conf))\n",
        "    ncor = np.sum(conf) - cor\n",
        "    print(\"Overall Accuracy: \", cor / (cor + ncor))\n",
        "    acc[snr] = 1.0 * cor / (cor + ncor)\n",
        "\n",
        "# %%Save results to a pickle file for plotting later\n",
        "print 'acc:', acc\n",
        "fd = open('results_%s_d0.5.dat' % (name), 'wb')\n",
        "cPickle.dump((\"%s\" % (name), 0.5, acc), fd)\n",
        "fd.close()\n",
        "print('oa:', oa_dict)\n",
        "fd = open('results_oa_%s_d0.5.dat' % (name), 'wb')\n",
        "cPickle.dump((\"%s\" % (name), 0.5, oa_dict), fd)\n",
        "fd.close()\n",
        "print('kappa:', kappa_dict)\n",
        "fd = open('results_kappa_%s_d0.5.dat' % (name), 'wb')\n",
        "cPickle.dump((\"%s\" % (name), 0.5, kappa_dict), fd)\n",
        "fd.close()\n",
        "\n",
        "# %%Plot accuracy curve\n",
        "plt.figure()\n",
        "plt.plot(snrs, list(map(lambda x: acc[x], snrs)))\n",
        "plt.xlabel(\"Signal to Noise Ratio\")\n",
        "plt.ylabel(\"Classification Accuracy\")\n",
        "plt.title(\"%s Classification Accuracy on RadioML 2016.10 Alpha\" % (name))\n",
        "plt.savefig(\"%s Classification Accuracy\" % (name))\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}