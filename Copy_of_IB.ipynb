{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of IB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kkuntal990/IB/blob/master/Copy_of_IB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBXDPxVXYwrD"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#import plotcm\n",
        "import sys\n",
        "import tensorflow.keras as keras\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Conv2D, ZeroPadding2D\n",
        "from tensorflow.keras.layers import Reshape, Dense, Dropout, Activation, Flatten, Lambda\n",
        "import tensorflow.keras.models as models\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UTbXN6Zbkf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdfb8409-9af8-43b3-d175-959823daf5d5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXDfrpJQaWj1"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/IB/RML2016.10a_dict.dat\" /content/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JQj5IZgYx-u"
      },
      "source": [
        "#import cPickle\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "sys.path.append('../confusion')\n",
        "\n",
        "name = 'CNN2'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm0o0AKrYund"
      },
      "source": [
        "with open(\"RML2016.10a_dict.dat\", 'rb') as xd1:  \n",
        "    Xd = pickle.load(xd1, encoding='latin1')  # , encoding='latin1'\n",
        "snrs, mods = map(lambda j: sorted(\n",
        "    list(set(map(lambda x: x[j], Xd.keys())))), [1, 0])\n",
        "X = []\n",
        "lbl = []\n",
        "SNR = []\n",
        " \n",
        "for mod in mods:\n",
        "    for snr in snrs:\n",
        "        SNR.append(snr)\n",
        "        X.append(Xd[(mod, snr)])\n",
        "        for i in range(Xd[(mod, snr)].shape[0]):\n",
        "            lbl.append((mod, snr))\n",
        "X = np.vstack(X)\n",
        "# %%\n",
        "np.random.seed(2016)  \n",
        "n_examples = X.shape[0]\n",
        "n_train = n_examples * 0.5  # ĺŻšĺ\n",
        "train_idx = np.random.choice(\n",
        "    range(0, n_examples), size=int(n_train), replace=False)\n",
        "test_idx = list(set(range(0, n_examples)) - set(train_idx))  # label\n",
        "test_idx2 = []\n",
        "X_train = X[train_idx]\n",
        "for i in test_idx:\n",
        "    if SNR[i//1000] >= 0:\n",
        "        test_idx2.append(i)\n",
        " \n",
        "X_test = X[test_idx2]\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzrJmCqJY5QA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57ecb9b-3425-40d7-fb2a-ad75142f9d54"
      },
      "source": [
        "def to_onehot(yy):\n",
        "    yy1 = np.zeros([len(yy), max(yy) + 1])\n",
        "    yy1[np.arange(len(yy)), yy] = 1\n",
        "    return yy1\n",
        "\n",
        "\n",
        "trainy = list(map(lambda x: mods.index(lbl[x][0]), train_idx))\n",
        "Y_train = to_onehot(trainy)\n",
        "Y_test = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx2)))\n",
        "# %%\n",
        "in_shp = list(X_train.shape[1:])\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(Y_train.shape, Y_test.shape)\n",
        "classes = mods\n",
        "print(in_shp)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(110000, 2, 128) (55027, 2, 128)\n",
            "(110000, 11) (55027, 11)\n",
            "[2, 128]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JzdfIa145yi"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow_probability import distributions as ds\n",
        "import math"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MUDQtBfZGh8"
      },
      "source": [
        "def sample_z(args):\n",
        "  mu, sigma = args\n",
        "  batch = K.shape(mu)[0]\n",
        "  dim = K.int_shape(mu)[1]\n",
        "  eps = K.random_normal(shape=(batch, dim))\n",
        "  return mu + K.log(1 + K.exp(sigma - 5.0)) * eps\n",
        "\n",
        "BETA = 1e-2\n",
        "prior = ds.Normal(0.0, 1.0)\n",
        "dr = 0.5"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tlzJUyF1Ffl"
      },
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        inp = keras.Input(shape=(2,128,))\n",
        "        x = Reshape(target_shape=(2,128,1,))(inp)\n",
        "        x = ZeroPadding2D((0,2), data_format=\"channels_first\")(x)\n",
        "        x = Conv2D(256, (1, 3), padding='valid', activation=\"relu\", name=\"conv1\",kernel_initializer='glorot_uniform')(x)\n",
        "        x = Dropout(dr)(x)\n",
        "        x = ZeroPadding2D((0, 2), data_format=\"channels_first\")(x)\n",
        "        x = Conv2D(80, (2, 3), padding=\"valid\", activation=\"relu\", name=\"conv2\",kernel_initializer='glorot_uniform')(x)\n",
        "        x = Dropout(dr)(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(512, activation='relu', name=\"mu\",kernel_initializer='he_normal')(x)\n",
        "        '''\n",
        "        mu = Dense(128, activation='relu', name=\"mu\")(x)\n",
        "        mu = Dropout(dr)(mu)\n",
        "\n",
        "        sigma = Dense(128, activation='relu', name=\"sigma\")(x)\n",
        "        sigma = Dropout(dr)(sigma)\n",
        "        '''\n",
        "        mu, sigma = x[:, :256], x[:, 256:]\n",
        "        #z = keras.layers.Lambda(sample_z, output_shape=(256, ), name='z')([mu, sigma])\n",
        "        encoder = keras.Model(inp, [mu,sigma], name=\"encoder\")\n",
        "        self.encoder = encoder\n",
        "        z = keras.Input(shape=(256,))\n",
        "        y = Dense(len(classes), name=\"dense3\", activation='relu',kernel_initializer='he_normal')(z)\n",
        "        decoder = keras.Model(z,y, name=\"decoder\")\n",
        "        self.decoder = decoder\n",
        "        self.encoder.summary()\n",
        "        self.decoder.summary()\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.val_loss_tracker = keras.metrics.Mean(name=\"val_loss\")\n",
        "        self.class_loss_tracker = keras.metrics.Mean(\n",
        "             name=\"class_loss\"\n",
        "         )\n",
        "        self.info_loss_tracker = keras.metrics.Mean(name=\"info_loss\")\n",
        "        self.IZY_bound_tracker = keras.metrics.Mean(name=\"IZY_bound\")\n",
        "        self.IZX_bound_tracker = keras.metrics.Mean(name=\"IZX_bound\")\n",
        "        self.train_acc_tracker = keras.metrics.Mean(name=\"Training accuracy\")\n",
        "        self.val_acc_tracker = keras.metrics.Mean(name=\"validation accuracy\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.class_loss_tracker,\n",
        "            self.info_loss_tracker,\n",
        "            self.total_loss_tracker,\n",
        "            self.IZY_bound_tracker,\n",
        "            self.IZX_bound_tracker\n",
        "            ,self.train_acc_tracker\n",
        "        ]\n",
        "\n",
        "    def call(self, data):\n",
        "      mu,sigma = self.encoder(data)\n",
        "      sigma = tf.math.softplus(sigma-5.0)\n",
        "      z_dist = ds.Normal(mu, sigma)\n",
        "      z = z_dist.sample()\n",
        "      y = self.decoder(z)\n",
        "      return tf.nn.softmax(y)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, inp):\n",
        "        data, y_true = inp\n",
        "        with tf.GradientTape() as tape:\n",
        "            mu,sigma = self.encoder(data)\n",
        "            sigma = tf.math.softplus(sigma-5.0)\n",
        "            z_dist = ds.Normal(mu, sigma)\n",
        "            z = z_dist.sample()\n",
        "            y = self.decoder(z)\n",
        "            \n",
        "          \n",
        "            info_loss = tf.reduce_sum(tf.reduce_mean(ds.kl_divergence(z_dist, prior), 0)) /math.log(2)\n",
        "            class_loss = tf.compat.v1.losses.softmax_cross_entropy(logits=y, onehot_labels=y_true)\n",
        "            total_loss =  class_loss + BETA*info_loss\n",
        "        training_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(y_true, axis=1)), tf.float32))\n",
        "        IZY_bound =  math.log(10, 2) - class_loss\n",
        "        IZX_bound = info_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.class_loss_tracker.update_state(class_loss)\n",
        "        self.info_loss_tracker.update_state(info_loss)\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.IZY_bound_tracker.update_state(IZY_bound)\n",
        "        self.IZX_bound_tracker.update_state(IZX_bound)\n",
        "        self.train_acc_tracker.update_state(training_acc)\n",
        "        \n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"info_loss\": self.info_loss_tracker.result(),\n",
        "            \"class_loss\": self.class_loss_tracker.result(),\n",
        "            \"IZY_bound\" : self.IZY_bound_tracker.result(),\n",
        "            \"IZX_bound\" : self.IZX_bound_tracker.result()\n",
        "            ,\"Train acc\" : self.train_acc_tracker.result()\n",
        "        }\n",
        "    @tf.function\n",
        "    def test_step(self, val_inp):\n",
        "        x_val, y_val = val_inp\n",
        "        mu,sigma = self.encoder(x_val)\n",
        "        sigma = tf.math.softplus(sigma-5)\n",
        "        z_dist = ds.Normal(mu, sigma)\n",
        "        z = z_dist.sample()\n",
        "        y = self.decoder(z)\n",
        "        info_loss = tf.reduce_sum(tf.reduce_mean(ds.kl_divergence(z_dist, prior), 0)) /math.log(2)\n",
        "        class_loss = tf.compat.v1.losses.softmax_cross_entropy(logits=y, onehot_labels=y_val)\n",
        "        val_loss =  class_loss + BETA*info_loss\n",
        "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf.nn.softmax(y), 1), tf.argmax(y_val, axis=1)), tf.float32))\n",
        "        self.val_loss_tracker.update_state(val_loss)\n",
        "        self.val_acc_tracker.update_state(val_acc)\n",
        "        return {\n",
        "            \"acc\" : self.val_acc_tracker.result(),\n",
        "            \"loss\" : self.val_loss_tracker.result()\n",
        "        }\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-ROMis14ZFo",
        "outputId": "01d7b34e-a706-4861-b419-d2c46c9e777a"
      },
      "source": [
        "vae = VAE()\n",
        "vae.compile(optimizer=keras.optimizers.Adam(lr=1e-4), metrics=[tf.keras.metrics.CategoricalAccuracy(name='categorical_accuracy')])\n",
        "history = vae.fit(X_train,Y_train,validation_data=(X_test, Y_test), epochs=10000, batch_size=1024, callbacks=[keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/IB_checkpoints', monitor='val_loss', verbose=0, save_best_only=True,\n",
        "                                                        mode='auto'),keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 2, 128)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 2, 128, 1)    0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_4 (ZeroPadding2D (None, 2, 128, 5)    0           reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 2, 126, 256)  4096        zero_padding2d_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 2, 126, 256)  0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPadding2D (None, 2, 126, 260)  0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2 (Conv2D)                  (None, 1, 124, 80)   124880      zero_padding2d_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 1, 124, 80)   0           conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 9920)         0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "mu (Dense)                      (None, 512)          5079552     flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_4 (Sli (None, 256)          0           mu[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_5 (Sli (None, 256)          0           mu[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 5,208,528\n",
            "Trainable params: 5,208,528\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 256)]             0         \n",
            "_________________________________________________________________\n",
            "dense3 (Dense)               (None, 11)                2827      \n",
            "=================================================================\n",
            "Total params: 2,827\n",
            "Trainable params: 2,827\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10000\n",
            "108/108 [==============================] - 7s 54ms/step - loss: 15.6458 - info_loss: 785.1193 - class_loss: 2.5494 - IZY_bound: 0.7725 - IZX_bound: 785.1193 - Train acc: 0.0998 - val_acc: 0.0915 - val_loss: 3.6250\n",
            "Epoch 2/10000\n",
            "108/108 [==============================] - 5s 50ms/step - loss: 3.6121 - info_loss: 95.0836 - class_loss: 2.6500 - IZY_bound: 0.6719 - IZX_bound: 95.0836 - Train acc: 0.0909 - val_acc: 0.0903 - val_loss: 3.6028\n",
            "Epoch 3/10000\n",
            "108/108 [==============================] - 5s 50ms/step - loss: 3.5696 - info_loss: 94.5498 - class_loss: 2.6147 - IZY_bound: 0.7072 - IZX_bound: 94.5498 - Train acc: 0.0895 - val_acc: 0.0910 - val_loss: 3.5833\n",
            "Epoch 4/10000\n",
            "108/108 [==============================] - 5s 50ms/step - loss: 3.5376 - info_loss: 94.4408 - class_loss: 2.5848 - IZY_bound: 0.7371 - IZX_bound: 94.4408 - Train acc: 0.0899 - val_acc: 0.0910 - val_loss: 3.5656\n",
            "Epoch 5/10000\n",
            "108/108 [==============================] - 5s 50ms/step - loss: 3.5080 - info_loss: 94.4491 - class_loss: 2.5544 - IZY_bound: 0.7675 - IZX_bound: 94.4491 - Train acc: 0.0915 - val_acc: 0.0911 - val_loss: 3.5499\n",
            "Epoch 6/10000\n",
            " 65/108 [=================>............] - ETA: 1s - loss: 3.4857 - info_loss: 94.5103 - class_loss: 2.5346 - IZY_bound: 0.7873 - IZX_bound: 94.5103 - Train acc: 0.0901"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUuGJBnywBjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d678e3ce-c39a-40ae-855f-3690bbdc8a0e"
      },
      "source": [
        "Y_pred = np.argmax(vae(X_test[:10000]),axis=1)\n",
        "Y_test2 = np.argmax(Y_test,axis=1)\n",
        "co = 0\n",
        "for i in range(10000):\n",
        "  if(Y_test2[i]==Y_pred[i]):\n",
        "    co+=1\n",
        "print(co)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef6MhrvuZo2H"
      },
      "source": [
        "# Show loss curves\n",
        "plt.figure()\n",
        "plt.title('Training performance')\n",
        "plt.plot(history.epoch, history.history['loss'], label='train loss+error')\n",
        "plt.plot(history.epoch, history.history['val_loss'], label='val_error')\n",
        "plt.legend()\n",
        "plt.savefig('%s Training performance' % (name))\n",
        "# plt.show()\n",
        "\n",
        "model.load_weights(filepath)\n",
        "score = model.evaluate(X_test, Y_test, verbose=0, batch_size=batch_size)\n",
        "print('evaluate_score:', score)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(labels))\n",
        "    plt.xticks(tick_marks, labels, rotation=45)\n",
        "    plt.yticks(tick_marks, labels)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.savefig(title)\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "# Plot confusion matrix\n",
        "test_Y_hat = model.predict(X_test, batch_size=batch_size)\n",
        "\n",
        "# %%č°ç¨ĺşäş§çćˇˇćˇçŠéľ\n",
        "pre_labels = []\n",
        "for x in test_Y_hat:\n",
        "    tmp = np.argmax(x, 0)\n",
        "    pre_labels.append(tmp)\n",
        "true_labels = []\n",
        "for x in Y_test:\n",
        "    tmp = np.argmax(x, 0)\n",
        "    true_labels.append(tmp)\n",
        "\n",
        "kappa = cohen_kappa_score(pre_labels, true_labels)\n",
        "oa = accuracy_score(true_labels, pre_labels)\n",
        "kappa_oa = {}\n",
        "print('oa_all:', oa)\n",
        "print('kappa_all:', kappa)\n",
        "kappa_oa['oa_all'] = oa\n",
        "kappa_oa['kappa_all'] = kappa\n",
        "fd = open('results_all_%s_d0.5.dat' % (name), 'wb')\n",
        "cPickle.dump((\"%s\" % (name), 0.5, kappa_oa), fd)\n",
        "fd.close()\n",
        "cnf_matrix = confusion_matrix(true_labels, pre_labels)\n",
        "# np.set_printoptions(precision=2)\n",
        "# Plot non-normalized confusion matrix\n",
        "# plt.figure()\n",
        "plotcm.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                             normalize=False,\n",
        "                             title='%s Confusion matrix, without normalization' % (name), showtext=True)\n",
        "plt.savefig('%s Confusion matrix, without normalization' % (name))\n",
        "# Plot normalized confusion matrix\n",
        "# plt.figure()\n",
        "plotcm.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                             normalize=True,\n",
        "                             title='%s Normalized confusion matrix' % (name), showtext=True)\n",
        "plt.savefig('%s Normalized confusion matrix' % (name))\n",
        "# plt.show()\n",
        "\n",
        "# %%čŞĺŽäšäş§çćˇˇćˇçŠéľ\n",
        "conf = np.zeros([len(classes), len(classes)])\n",
        "confnorm = np.zeros([len(classes), len(classes)])\n",
        "for i in range(0, X_test.shape[0]):\n",
        "    j = list(Y_test[i, :]).index(1)\n",
        "    k = int(np.argmax(test_Y_hat[i, :]))\n",
        "    conf[j, k] += 1\n",
        "for i in range(0, len(classes)):\n",
        "    confnorm[i, :] = conf[i, :] / np.sum(conf[i, :])\n",
        "plot_confusion_matrix(confnorm, labels=classes,\n",
        "                      title='%s Confusion matrix' % (name))\n",
        "\n",
        "# %%Plot confusion matrix çťĺž\n",
        "acc = {}\n",
        "kappa_dict = {}\n",
        "oa_dict = {}\n",
        "for snr in snrs:\n",
        "\n",
        "    # extract classes @ SNR\n",
        "    test_SNRs = list(map(lambda x: lbl[x][1], test_idx))\n",
        "    test_X_i = X_test[np.where(np.array(test_SNRs) == snr)]\n",
        "    test_Y_i = Y_test[np.where(np.array(test_SNRs) == snr)]\n",
        "\n",
        "    # estimate classes\n",
        "    test_Y_i_hat = model.predict(test_X_i)\n",
        "\n",
        "    # %%č°ç¨ĺşäş§çćˇˇćˇçŠéľ\n",
        "    pre_labels_i = []\n",
        "    for x in test_Y_i_hat:\n",
        "        tmp = np.argmax(x, 0)\n",
        "        pre_labels_i.append(tmp)\n",
        "    true_labels_i = []\n",
        "    for x in test_Y_i:\n",
        "        tmp = np.argmax(x, 0)\n",
        "        true_labels_i.append(tmp)\n",
        "    kappa = cohen_kappa_score(pre_labels_i, true_labels_i)\n",
        "    oa = accuracy_score(true_labels_i, pre_labels_i)\n",
        "    oa_dict[snr] = oa\n",
        "    kappa_dict[snr] = kappa\n",
        "    cnf_matrix = confusion_matrix(true_labels_i, pre_labels_i)\n",
        "    # np.set_printoptions(precision=2)\n",
        "    # Plot non-normalized confusion matrix\n",
        "    # plt.figure()\n",
        "    plotcm.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                                 normalize=False,\n",
        "                                 title='%s Confusion matrix, without normalization (SNR=%d)' % (name, snr), showtext=True)\n",
        "    plt.savefig('%s Confusion matrix, without normalization (SNR=%d)' %\n",
        "                (name, snr))\n",
        "    # Plot normalized confusion matrix\n",
        "    # plt.figure()\n",
        "    plotcm.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                                 normalize=True,\n",
        "                                 title='%s Normalized confusion matrix (SNR=%d)' % (name, snr), showtext=True)\n",
        "    plt.savefig('%s Normalized confusion matrix (SNR=%d)' % (name, snr))\n",
        "    # plt.show()\n",
        "\n",
        "    # %%čŞĺŽäšäş§çćˇˇćˇçŠéľ\n",
        "    conf = np.zeros([len(classes), len(classes)])\n",
        "    confnorm = np.zeros([len(classes), len(classes)])\n",
        "    for i in range(0, test_X_i.shape[0]):\n",
        "        j = list(test_Y_i[i, :]).index(1)\n",
        "        k = int(np.argmax(test_Y_i_hat[i, :]))\n",
        "        conf[j, k] += 1\n",
        "    for i in range(0, len(classes)):\n",
        "        confnorm[i, :] = conf[i, :] / np.sum(conf[i, :])\n",
        "    # plt.figure()\n",
        "    plot_confusion_matrix(confnorm, labels=classes,\n",
        "                          title=\"%s Confusion Matrix (SNR=%d)\" % (name, snr))\n",
        "\n",
        "    cor = np.sum(np.diag(conf))\n",
        "    ncor = np.sum(conf) - cor\n",
        "    print(\"Overall Accuracy: \", cor / (cor + ncor))\n",
        "    acc[snr] = 1.0 * cor / (cor + ncor)\n",
        "\n",
        "# %%Save results to a pickle file for plotting later\n",
        "print 'acc:', acc\n",
        "fd = open('results_%s_d0.5.dat' % (name), 'wb')\n",
        "cPickle.dump((\"%s\" % (name), 0.5, acc), fd)\n",
        "fd.close()\n",
        "print('oa:', oa_dict)\n",
        "fd = open('results_oa_%s_d0.5.dat' % (name), 'wb')\n",
        "cPickle.dump((\"%s\" % (name), 0.5, oa_dict), fd)\n",
        "fd.close()\n",
        "print('kappa:', kappa_dict)\n",
        "fd = open('results_kappa_%s_d0.5.dat' % (name), 'wb')\n",
        "cPickle.dump((\"%s\" % (name), 0.5, kappa_dict), fd)\n",
        "fd.close()\n",
        "\n",
        "# %%Plot accuracy curve\n",
        "plt.figure()\n",
        "plt.plot(snrs, list(map(lambda x: acc[x], snrs)))\n",
        "plt.xlabel(\"Signal to Noise Ratio\")\n",
        "plt.ylabel(\"Classification Accuracy\")\n",
        "plt.title(\"%s Classification Accuracy on RadioML 2016.10 Alpha\" % (name))\n",
        "plt.savefig(\"%s Classification Accuracy\" % (name))\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}